{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## trainer.ipynb\n",
    "\n",
    "This notebook is used to train the model described in \"Deep Imitation Learning for Complex Manipulation Tasks from Virtual Reality Teleoperation\" (https://arxiv.org/abs/1710.04615)\n",
    "\n",
    "Make sure the /data/ dataset folder is in the same directory as this notebook.  Dataset format:\n",
    "\n",
    "/data/\n",
    " - /{runNumber}/\n",
    "   - /depth/ - contains 1 channel depth images\n",
    "      - fileNames: depth{stepNum}.png\n",
    "   - /rgb/ - contains 3 channel rgb images\n",
    "      - fileNames: rgb{stepNum}.png\n",
    "   - /states/ - contains csv files of the format:\n",
    "      - endEffectorPt1X, endEffectorPt1Y, endEffectorPt2X, endEffectorPt2Y, endEffectorPt3X, endEffectorPt3Y, isOpen (boolean: {0 = closed, 1 = open})\n",
    "      - endEffectorX, endEffectorY, endEffectorZ, endEffectorRoll, endEffectorPitch, endEffectorYaw\n",
    "      - fileNames: states{stepNum}.csv\n",
    "\n",
    "Note that for simplicity in this implementation, the network is not trained using auxilary points.  Further, the network only considers the linear movement of the end effector (no roll / pitch / yaw control.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from VRNet import VRNet\n",
    "from VRNet import VRDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = VRDataLoader('data', 2, 160, batch_size=128)\n",
    "\n",
    "states = dataloader.states\n",
    "\n",
    "#set outliers to 0 (where |x| > 1)\n",
    "for i in range(len(states)):\n",
    "    if abs(states[i][0]) > 1:\n",
    "        #interpolate between two adjacent states\n",
    "        #if either value adjacent is out of bounds, use the other\n",
    "        if i == 0:\n",
    "            states[i][0] = states[i+1][0]\n",
    "        elif i == states.shape[0] - 1:\n",
    "            states[i][0] = states[i-1][0]\n",
    "        else:\n",
    "            states[i][0] = (states[i-1][0] + states[i+1][0]) / 2\n",
    "    if abs(states[i][1]) > 1:\n",
    "        if i == 0:\n",
    "            states[i][1] = states[i+1][1]\n",
    "        elif i == states.shape[0] - 1:\n",
    "            states[i][1] = states[i-1][1]\n",
    "        else:\n",
    "            states[i][1] = (states[i-1][1] + states[i+1][1]) / 2\n",
    "    if abs(states[i][2]) > 1:\n",
    "        if i == 0:\n",
    "            states[i][2] = states[i+1][2]\n",
    "        elif i == states.shape[0] - 1:\n",
    "            states[i][2] = states[i-1][2]\n",
    "        else:\n",
    "            states[i][2] = (states[i-1][2] + states[i+1][2]) / 2\n",
    "\n",
    "#apply a gaussian filter to smooth out the data\n",
    "gaussian_filter = np.array([1, 2, 3, 4, 5, 4, 3, 2, 1])\n",
    "gaussian_filter = gaussian_filter / np.sum(gaussian_filter)\n",
    "\n",
    "states = states.cpu().numpy()\n",
    "for i in range(6):\n",
    "    states[:, i] = np.convolve(states[:, i], gaussian_filter, mode='same')\n",
    "states = torch.tensor(states).to('cuda')\n",
    "\n",
    "#plot x velocities\n",
    "plt.plot([state[0].cpu() for state in states][0:500], 'bo')\n",
    "plt.show()\n",
    "\n",
    "#plot y velocities\n",
    "plt.plot([state[1].cpu() for state in states][0:500], 'bo')\n",
    "plt.show()\n",
    "\n",
    "#plot z velocities\n",
    "plt.plot([state[2].cpu() for state in states][0:500], 'bo')\n",
    "plt.show()\n",
    "\n",
    "#display image\n",
    "plt.imshow(dataloader.rgb_images[0].permute(1,2,0))\n",
    "plt.show()\n",
    "#display histogram of image\n",
    "plt.hist(dataloader.rgb_images[0].permute(1,2,0).flatten().cpu().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#show a random img from the dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "idx = 0 #random.randint(0, len(dataloader))\n",
    "data = dataloader[idx]\n",
    "rgb_img, depth_img, state = data[0][0], data[1][0], data[2][0]\n",
    "\n",
    "plt.imshow(rgb_img.permute(1, 2, 0))\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(depth_img.permute(1, 2, 0))\n",
    "plt.show()\n",
    "\n",
    "print(state)\n",
    "\n",
    "print(len(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add data augmentation\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.RandomAffine(0, shear=3),\n",
    "    torchvision.transforms.RandomAffine(0, scale=(0.98, 1.02)),\n",
    "    # torchvision.transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.05, hue=0.05),\n",
    "    # torchvision.transforms.RandomApply([torchvision.transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0))], p=0.5),\n",
    "])\n",
    "\n",
    "def applyTransforms(rgb_img, depth_img):\n",
    "    rngstate = torch.random.get_rng_state()\n",
    "    rgb_img = transform(rgb_img)\n",
    "    torch.random.set_rng_state(rngstate)\n",
    "    depth_img = transform(depth_img)\n",
    "    return rgb_img, depth_img\n",
    "\n",
    "idx = random.randint(0, len(dataloader))\n",
    "data = dataloader[idx]\n",
    "rgb_img, depth_img, state = data[0][1], data[1][1], data[2][1]\n",
    "\n",
    "rgb_img, depth_img = applyTransforms(rgb_img, depth_img)\n",
    "\n",
    "#apply same transform to both depth and rgb image\n",
    "plt.imshow(rgb_img.permute(1, 2, 0))\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(depth_img.permute(1, 2, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the custom loss functions used by the paper\n",
    "\n",
    "#create custom Lc loss function\n",
    "class LcLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LcLoss, self).__init__()\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        loss = torch.zeros(pred.shape[0])\n",
    "        for i in range(pred.shape[0]):\n",
    "            p = pred[i]\n",
    "            t = target[i]\n",
    "            loss[i] = torch.arccos(torch.dot(t, p) / (torch.norm(t) * torch.norm(p)))\n",
    "        \n",
    "        return torch.sum(loss)\n",
    "\n",
    "#create custom Lg loss function\n",
    "class LgLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LgLoss, self).__init__()\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        loss = torch.zeros(pred.shape[0])\n",
    "        for i in range(pred.shape[0]):\n",
    "            p = pred[i]\n",
    "            t = target[i]\n",
    "            loss[i] = p * torch.log(t) - (1 - p) * torch.log(1 - t)\n",
    "        return torch.sum(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_weights = [1, 0.01, 0.005, 0.0001]\n",
    "\n",
    "def train(model, data_loader, num_epochs, learning_rate, device):\n",
    "    model.train()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.0001)\n",
    "    # optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    L1_loss = nn.L1Loss()\n",
    "    L2_loss = nn.MSELoss()\n",
    "    L_c_loss = LcLoss()\n",
    "    L_g_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i in range(len(data_loader)):\n",
    "            rgb_img, depth_img, state = data_loader[i]\n",
    "            rgb_img = rgb_img\n",
    "            depth_img = depth_img\n",
    "\n",
    "            #get rgb and depth images\n",
    "            rgb_img = rgb_img.to(device).float()\n",
    "            depth_img = depth_img.to(device).float()\n",
    "\n",
    "            \n",
    "            #apply data augmentation\n",
    "            # rgb_img, depth_img = applyTransforms(rgb_img, depth_img)\n",
    "            \n",
    "            #add batch dimension\n",
    "            state = state.to(device).float()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(rgb_img, depth_img)\n",
    "            \n",
    "            #calculate combined loss\n",
    "            # loss = L1_loss(output[0:3], state[0:3]) * loss_weights[0]\n",
    "            #combine 0:3 and 6\n",
    "            important_output = torch.cat((output[:, 0:3], output[:, 6].unsqueeze(1)), dim=1)\n",
    "            important_state = torch.cat((state[:, 0:3], state[:, 6].unsqueeze(1)), dim=1)\n",
    "            \n",
    "            loss = L1_loss(important_output, important_state) # * loss_weights[1]\n",
    "            # loss += L_c_loss(output[:, 0:6], state[:, 0:6]) * loss_weights[2]\n",
    "            # loss += L_g_loss(output[:, 6], state[:, 6]) * loss_weights[3]\n",
    "            \n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f'Epoch: {epoch}, Iteration: {i}, Loss: {loss.item()}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model\n",
    "device = torch.device('cuda')\n",
    "model = VRNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, dataloader, 500, 0.0005, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test model on an example image\n",
    "import numpy as np\n",
    "\n",
    "idx = random.randint(0, len(dataloader))\n",
    "rgb_img, depth_img, state = dataloader[idx][0][0], dataloader[idx][1][0], dataloader[idx][2][0]\n",
    "\n",
    "model.eval()\n",
    "rgb_img = rgb_img.unsqueeze(0).to(device).float() \n",
    "depth_img = depth_img.unsqueeze(0).to(device).float()\n",
    "\n",
    "rgb_img = rgb_img.permute(0, 1, 2, 3)\n",
    "depth_img = depth_img.permute(0, 1, 2, 3)\n",
    "\n",
    "print(rgb_img.shape, depth_img.shape)\n",
    "output = model(rgb_img, depth_img)\n",
    "\n",
    "#show output (no scientific notation)\n",
    "print('output: {} {} {} {}'.format(output[0][0].item(), output[0][1].item(), output[0][2].item(), output[0][6].item(),))\n",
    "print('mse: ', np.mean((output.detach().cpu().numpy()[0:2] - state.detach().cpu().numpy())[0:2] ** 2))\n",
    "print(state)\n",
    "\n",
    "print(rgb_img.shape)\n",
    "plt.imshow(rgb_img.permute(0, 2, 3, 1).cpu()[0])\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(depth_img.permute(0, 2, 3, 1).cpu()[0])\n",
    "plt.show()\n",
    "\n",
    "print(state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model\n",
    "torch.save(model.state_dict(), 'model.pt')\n",
    "\n",
    "#load the model\n",
    "model = VRNet().to(device)\n",
    "model.load_state_dict(torch.load('model.pt'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pybullet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "06b11139e13515c2711c878e587d72e1e158f858f2134781b554bf5f2a3bd4a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
